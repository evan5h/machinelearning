{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached https://files.pythonhosted.org/packages/52/d8/1a966940585bdd828d6ca8bca37d1be81e3e7e2fa1f51098117f15c32a1b/gensim-3.6.0-cp36-cp36m-win_amd64.whl\n",
      "Requirement already satisfied: scipy>=0.18.1 in e:\\anaconda\\lib\\site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5.0 in e:\\anaconda\\lib\\site-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in e:\\anaconda\\lib\\site-packages (from gensim) (1.14.3)\n",
      "Collecting smart-open>=1.2.1 (from gensim)\n",
      "Requirement already satisfied: boto>=2.32 in e:\\anaconda\\lib\\site-packages (from smart-open>=1.2.1->gensim) (2.48.0)\n",
      "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/0b/3b/925c7aded1c54c12d5be964ab543420b01d95759c5756a574588d49444ad/boto3-1.9.71-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests in e:\\anaconda\\lib\\site-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
      "Collecting bz2file (from smart-open>=1.2.1->gensim)\n",
      "Collecting botocore<1.13.0,>=1.12.71 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/50/c0/cd4f8bec8a10876f0ce34f0cf264fda04e09df41d1a473a43f890c71fffa/botocore-1.12.71-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in e:\\anaconda\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in e:\\anaconda\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in e:\\anaconda\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\anaconda\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim) (2018.4.16)\n",
      "Requirement already satisfied: docutils>=0.10 in e:\\anaconda\\lib\\site-packages (from botocore<1.13.0,>=1.12.71->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in e:\\anaconda\\lib\\site-packages (from botocore<1.13.0,>=1.12.71->boto3->smart-open>=1.2.1->gensim) (2.7.3)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, bz2file, smart-open, gensim\n",
      "Successfully installed boto3-1.9.71 botocore-1.12.71 bz2file-0.98 gensim-3.6.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.7.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "!{sys.executable} -m pip install gensim\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = BeautifulSoup(data[\"review\"][0], \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters_only = re.sub(\"[^a-zA-Z-']\", \n",
    "                      \" \",\n",
    "                      bs.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase = letters_only.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lowercase.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(raw):\n",
    "    data = BeautifulSoup(raw, \"lxml\").get_text() \n",
    "    letters_only = re.sub(\"[^a-zA-Z-']\", \" \", data) \n",
    "    \n",
    "    words = letters_only.lower().split()\n",
    "    return( \" \".join( words )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cleaner = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"review\"] = data[\"review\"].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = \"english\", max_features = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features = vectorizer.fit_transform(data[\"review\"])\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = forest.fit( train_data_features, data[\"sentiment\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"testData.tsv\", header=0, delimiter = \"\\t\", quoting = 3)\n",
    "result = []\n",
    "for review in test[\"review\"]:\n",
    "    result.append(forest.predict(vectorizer.transform([clean(review)]).toarray())[0])\n",
    "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
    "output.to_csv( \"Bag_of_Words_model.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 25000labeled train reviews, 25000test reviews, and  50000 unlabeled reviews.\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter = \"\\t\", quoting=3)\n",
    "udata = pd.read_csv(\"unlabeledTrainData.tsv\", header=0, delimiter = \"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"testData.tsv\", header=0, delimiter = \"\\t\", quoting = 3)\n",
    "\n",
    "print (\"read \" + str(data[\"review\"].size) + \"labeled train reviews, \" + str(test[\"review\"].size) + \"test reviews, and \", str(udata[\"review\"].size) + \" unlabeled reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\evan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers\\punkt\\english.pickle')\n",
    "def sentences (review, tokenizer):\n",
    "    raw_sentences = tokenizer.tokenize(review)\n",
    "    i=0\n",
    "    for unclean_sentence in raw_sentences:\n",
    "        raw_sentences[i] = clean(raw_sentences[i])\n",
    "        i = i + 1\n",
    "    sentences = [] \n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentence = raw_sentence.split()\n",
    "            sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "E:\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "E:\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentenceList = []\n",
    "for review in data[\"review\"]:\n",
    "    sentenceList += sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "E:\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "E:\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "E:\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'... ...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "E:\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "E:\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'....'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "E:\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "E:\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "E:\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "E:\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.. .'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "E:\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for review in udata[\"review\"]:\n",
    "    sentenceList += sentences(review, tokenizer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2019-01-01 21:07:55,750 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2019-01-01 21:07:55,760 : INFO : collecting all words and their counts\n",
      "2019-01-01 21:07:55,761 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-01-01 21:07:55,813 : INFO : PROGRESS: at sentence #10000, processed 220735 words, keeping 19851 word types\n",
      "2019-01-01 21:07:55,867 : INFO : PROGRESS: at sentence #20000, processed 442224 words, keeping 28867 word types\n",
      "2019-01-01 21:07:55,917 : INFO : PROGRESS: at sentence #30000, processed 657642 words, keeping 35592 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-01 21:07:55,972 : INFO : PROGRESS: at sentence #40000, processed 879534 words, keeping 41421 word types\n",
      "2019-01-01 21:07:56,028 : INFO : PROGRESS: at sentence #50000, processed 1099840 words, keeping 46418 word types\n",
      "2019-01-01 21:07:56,076 : INFO : PROGRESS: at sentence #60000, processed 1316019 words, keeping 50725 word types\n",
      "2019-01-01 21:07:56,122 : INFO : PROGRESS: at sentence #70000, processed 1536098 words, keeping 54692 word types\n",
      "2019-01-01 21:07:56,171 : INFO : PROGRESS: at sentence #80000, processed 1749981 words, keeping 58290 word types\n",
      "2019-01-01 21:07:56,224 : INFO : PROGRESS: at sentence #90000, processed 1968271 words, keeping 62087 word types\n",
      "2019-01-01 21:07:56,279 : INFO : PROGRESS: at sentence #100000, processed 2185268 words, keeping 65279 word types\n",
      "2019-01-01 21:07:56,335 : INFO : PROGRESS: at sentence #110000, processed 2401784 words, keeping 68422 word types\n",
      "2019-01-01 21:07:56,391 : INFO : PROGRESS: at sentence #120000, processed 2620140 words, keeping 71619 word types\n",
      "2019-01-01 21:07:56,443 : INFO : PROGRESS: at sentence #130000, processed 2839129 words, keeping 74475 word types\n",
      "2019-01-01 21:07:56,491 : INFO : PROGRESS: at sentence #140000, processed 3055910 words, keeping 77171 word types\n",
      "2019-01-01 21:07:56,544 : INFO : PROGRESS: at sentence #150000, processed 3275758 words, keeping 79967 word types\n",
      "2019-01-01 21:07:56,596 : INFO : PROGRESS: at sentence #160000, processed 3494473 words, keeping 82577 word types\n",
      "2019-01-01 21:07:56,646 : INFO : PROGRESS: at sentence #170000, processed 3713532 words, keeping 85072 word types\n",
      "2019-01-01 21:07:56,700 : INFO : PROGRESS: at sentence #180000, processed 3930537 words, keeping 87435 word types\n",
      "2019-01-01 21:07:56,757 : INFO : PROGRESS: at sentence #190000, processed 4152264 words, keeping 89775 word types\n",
      "2019-01-01 21:07:56,811 : INFO : PROGRESS: at sentence #200000, processed 4371389 words, keeping 92004 word types\n",
      "2019-01-01 21:07:56,865 : INFO : PROGRESS: at sentence #210000, processed 4589975 words, keeping 94301 word types\n",
      "2019-01-01 21:07:56,918 : INFO : PROGRESS: at sentence #220000, processed 4810611 words, keeping 96621 word types\n",
      "2019-01-01 21:07:56,978 : INFO : PROGRESS: at sentence #230000, processed 5029258 words, keeping 98868 word types\n",
      "2019-01-01 21:07:57,026 : INFO : PROGRESS: at sentence #240000, processed 5250160 words, keeping 101059 word types\n",
      "2019-01-01 21:07:57,073 : INFO : PROGRESS: at sentence #250000, processed 5457767 words, keeping 103185 word types\n",
      "2019-01-01 21:07:57,123 : INFO : PROGRESS: at sentence #260000, processed 5675836 words, keeping 105310 word types\n",
      "2019-01-01 21:07:57,174 : INFO : PROGRESS: at sentence #270000, processed 5893473 words, keeping 107609 word types\n",
      "2019-01-01 21:07:57,225 : INFO : PROGRESS: at sentence #280000, processed 6115026 words, keeping 110295 word types\n",
      "2019-01-01 21:07:57,275 : INFO : PROGRESS: at sentence #290000, processed 6334908 words, keeping 112626 word types\n",
      "2019-01-01 21:07:57,331 : INFO : PROGRESS: at sentence #300000, processed 6557829 words, keeping 114949 word types\n",
      "2019-01-01 21:07:57,381 : INFO : PROGRESS: at sentence #310000, processed 6775152 words, keeping 117237 word types\n",
      "2019-01-01 21:07:57,433 : INFO : PROGRESS: at sentence #320000, processed 6997909 words, keeping 119502 word types\n",
      "2019-01-01 21:07:57,485 : INFO : PROGRESS: at sentence #330000, processed 7217790 words, keeping 121619 word types\n",
      "2019-01-01 21:07:57,539 : INFO : PROGRESS: at sentence #340000, processed 7440387 words, keeping 123793 word types\n",
      "2019-01-01 21:07:57,593 : INFO : PROGRESS: at sentence #350000, processed 7658370 words, keeping 125911 word types\n",
      "2019-01-01 21:07:57,643 : INFO : PROGRESS: at sentence #360000, processed 7878381 words, keeping 127900 word types\n",
      "2019-01-01 21:07:57,694 : INFO : PROGRESS: at sentence #370000, processed 8099416 words, keeping 129965 word types\n",
      "2019-01-01 21:07:57,748 : INFO : PROGRESS: at sentence #380000, processed 8319712 words, keeping 131964 word types\n",
      "2019-01-01 21:07:57,802 : INFO : PROGRESS: at sentence #390000, processed 8543063 words, keeping 133913 word types\n",
      "2019-01-01 21:07:57,849 : INFO : PROGRESS: at sentence #400000, processed 8761022 words, keeping 135867 word types\n",
      "2019-01-01 21:07:57,903 : INFO : PROGRESS: at sentence #410000, processed 8977498 words, keeping 137697 word types\n",
      "2019-01-01 21:07:57,951 : INFO : PROGRESS: at sentence #420000, processed 9195490 words, keeping 139600 word types\n",
      "2019-01-01 21:07:58,002 : INFO : PROGRESS: at sentence #430000, processed 9418969 words, keeping 141534 word types\n",
      "2019-01-01 21:07:58,053 : INFO : PROGRESS: at sentence #440000, processed 9636966 words, keeping 143343 word types\n",
      "2019-01-01 21:07:58,105 : INFO : PROGRESS: at sentence #450000, processed 9859192 words, keeping 145237 word types\n",
      "2019-01-01 21:07:58,166 : INFO : PROGRESS: at sentence #460000, processed 10086841 words, keeping 147093 word types\n",
      "2019-01-01 21:07:58,224 : INFO : PROGRESS: at sentence #470000, processed 10309884 words, keeping 148750 word types\n",
      "2019-01-01 21:07:58,270 : INFO : PROGRESS: at sentence #480000, processed 10527046 words, keeping 150525 word types\n",
      "2019-01-01 21:07:58,322 : INFO : PROGRESS: at sentence #490000, processed 10748637 words, keeping 152274 word types\n",
      "2019-01-01 21:07:58,373 : INFO : PROGRESS: at sentence #500000, processed 10966001 words, keeping 154005 word types\n",
      "2019-01-01 21:07:58,427 : INFO : PROGRESS: at sentence #510000, processed 11186547 words, keeping 155685 word types\n",
      "2019-01-01 21:07:58,477 : INFO : PROGRESS: at sentence #520000, processed 11406284 words, keeping 157439 word types\n",
      "2019-01-01 21:07:58,529 : INFO : PROGRESS: at sentence #530000, processed 11628212 words, keeping 159155 word types\n",
      "2019-01-01 21:07:58,574 : INFO : PROGRESS: at sentence #540000, processed 11843512 words, keeping 160920 word types\n",
      "2019-01-01 21:07:58,632 : INFO : PROGRESS: at sentence #550000, processed 12065213 words, keeping 162607 word types\n",
      "2019-01-01 21:07:58,682 : INFO : PROGRESS: at sentence #560000, processed 12285182 words, keeping 164294 word types\n",
      "2019-01-01 21:07:58,735 : INFO : PROGRESS: at sentence #570000, processed 12507483 words, keeping 166013 word types\n",
      "2019-01-01 21:07:58,787 : INFO : PROGRESS: at sentence #580000, processed 12724493 words, keeping 167616 word types\n",
      "2019-01-01 21:07:58,841 : INFO : PROGRESS: at sentence #590000, processed 12943787 words, keeping 169149 word types\n",
      "2019-01-01 21:07:58,887 : INFO : PROGRESS: at sentence #600000, processed 13159732 words, keeping 170737 word types\n",
      "2019-01-01 21:07:58,939 : INFO : PROGRESS: at sentence #610000, processed 13381053 words, keeping 172376 word types\n",
      "2019-01-01 21:07:58,990 : INFO : PROGRESS: at sentence #620000, processed 13603645 words, keeping 173945 word types\n",
      "2019-01-01 21:07:59,053 : INFO : PROGRESS: at sentence #630000, processed 13818744 words, keeping 175470 word types\n",
      "2019-01-01 21:07:59,102 : INFO : PROGRESS: at sentence #640000, processed 14036045 words, keeping 176985 word types\n",
      "2019-01-01 21:07:59,149 : INFO : PROGRESS: at sentence #650000, processed 14256309 words, keeping 178551 word types\n",
      "2019-01-01 21:07:59,201 : INFO : PROGRESS: at sentence #660000, processed 14474700 words, keeping 180047 word types\n",
      "2019-01-01 21:07:59,255 : INFO : PROGRESS: at sentence #670000, processed 14696684 words, keeping 181458 word types\n",
      "2019-01-01 21:07:59,310 : INFO : PROGRESS: at sentence #680000, processed 14914744 words, keeping 182938 word types\n",
      "2019-01-01 21:07:59,366 : INFO : PROGRESS: at sentence #690000, processed 15133608 words, keeping 184508 word types\n",
      "2019-01-01 21:07:59,421 : INFO : PROGRESS: at sentence #700000, processed 15358161 words, keeping 185953 word types\n",
      "2019-01-01 21:07:59,474 : INFO : PROGRESS: at sentence #710000, processed 15579320 words, keeping 187315 word types\n",
      "2019-01-01 21:07:59,527 : INFO : PROGRESS: at sentence #720000, processed 15797588 words, keeping 188672 word types\n",
      "2019-01-01 21:07:59,578 : INFO : PROGRESS: at sentence #730000, processed 16016025 words, keeping 190032 word types\n",
      "2019-01-01 21:07:59,629 : INFO : PROGRESS: at sentence #740000, processed 16235443 words, keeping 191413 word types\n",
      "2019-01-01 21:07:59,679 : INFO : PROGRESS: at sentence #750000, processed 16449956 words, keeping 192770 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-01 21:07:59,726 : INFO : PROGRESS: at sentence #760000, processed 16665913 words, keeping 194126 word types\n",
      "2019-01-01 21:07:59,780 : INFO : PROGRESS: at sentence #770000, processed 16889304 words, keeping 195567 word types\n",
      "2019-01-01 21:07:59,833 : INFO : PROGRESS: at sentence #780000, processed 17113405 words, keeping 196965 word types\n",
      "2019-01-01 21:07:59,888 : INFO : PROGRESS: at sentence #790000, processed 17334420 words, keeping 198409 word types\n",
      "2019-01-01 21:07:59,895 : INFO : collected 198569 word types from a corpus of 17359734 raw words and 791168 sentences\n",
      "2019-01-01 21:07:59,896 : INFO : Loading a fresh vocabulary\n",
      "2019-01-01 21:08:00,000 : INFO : effective_min_count=40 retains 16710 unique words (8% of original 198569, drops 181859)\n",
      "2019-01-01 21:08:00,000 : INFO : effective_min_count=40 leaves 16638542 word corpus (95% of original 17359734, drops 721192)\n",
      "2019-01-01 21:08:00,064 : INFO : deleting the raw counts dictionary of 198569 items\n",
      "2019-01-01 21:08:00,071 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2019-01-01 21:08:00,071 : INFO : downsampling leaves estimated 12390547 word corpus (74.5% of prior 16638542)\n",
      "2019-01-01 21:08:00,147 : INFO : estimated required memory for 16710 words and 300 dimensions: 48459000 bytes\n",
      "2019-01-01 21:08:00,148 : INFO : resetting layer weights\n",
      "2019-01-01 21:08:00,405 : INFO : training model with 4 workers on 16710 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-01-01 21:08:01,421 : INFO : EPOCH 1 - PROGRESS: at 7.17% examples, 883394 words/s, in_qsize 6, out_qsize 1\n",
      "2019-01-01 21:08:02,449 : INFO : EPOCH 1 - PROGRESS: at 13.93% examples, 843535 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:03,454 : INFO : EPOCH 1 - PROGRESS: at 20.85% examples, 846022 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:04,455 : INFO : EPOCH 1 - PROGRESS: at 28.28% examples, 863772 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:05,459 : INFO : EPOCH 1 - PROGRESS: at 35.53% examples, 868128 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:06,470 : INFO : EPOCH 1 - PROGRESS: at 43.24% examples, 882200 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:07,471 : INFO : EPOCH 1 - PROGRESS: at 50.29% examples, 881215 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:08,482 : INFO : EPOCH 1 - PROGRESS: at 58.06% examples, 890873 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:09,508 : INFO : EPOCH 1 - PROGRESS: at 65.57% examples, 892989 words/s, in_qsize 6, out_qsize 1\n",
      "2019-01-01 21:08:10,507 : INFO : EPOCH 1 - PROGRESS: at 72.27% examples, 887064 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:11,514 : INFO : EPOCH 1 - PROGRESS: at 79.95% examples, 891966 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:12,528 : INFO : EPOCH 1 - PROGRESS: at 87.92% examples, 899124 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:13,540 : INFO : EPOCH 1 - PROGRESS: at 95.80% examples, 903706 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:14,084 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-01 21:08:14,087 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-01 21:08:14,101 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-01 21:08:14,107 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-01 21:08:14,108 : INFO : EPOCH - 1 : training on 17359734 raw words (12390399 effective words) took 13.7s, 904794 effective words/s\n",
      "2019-01-01 21:08:15,124 : INFO : EPOCH 2 - PROGRESS: at 7.24% examples, 890942 words/s, in_qsize 6, out_qsize 1\n",
      "2019-01-01 21:08:16,136 : INFO : EPOCH 2 - PROGRESS: at 14.79% examples, 902704 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:17,138 : INFO : EPOCH 2 - PROGRESS: at 22.41% examples, 914566 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:18,139 : INFO : EPOCH 2 - PROGRESS: at 29.99% examples, 920662 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-01 21:08:19,145 : INFO : EPOCH 2 - PROGRESS: at 37.30% examples, 915174 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:20,146 : INFO : EPOCH 2 - PROGRESS: at 43.87% examples, 899130 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:21,147 : INFO : EPOCH 2 - PROGRESS: at 51.69% examples, 908863 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:22,153 : INFO : EPOCH 2 - PROGRESS: at 59.41% examples, 915816 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:23,159 : INFO : EPOCH 2 - PROGRESS: at 67.16% examples, 920271 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:24,165 : INFO : EPOCH 2 - PROGRESS: at 75.33% examples, 928867 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:25,165 : INFO : EPOCH 2 - PROGRESS: at 83.65% examples, 937697 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:26,178 : INFO : EPOCH 2 - PROGRESS: at 91.60% examples, 941108 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-01 21:08:27,186 : INFO : EPOCH 2 - PROGRESS: at 98.78% examples, 936643 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:27,316 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-01 21:08:27,320 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-01 21:08:27,330 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-01 21:08:27,336 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-01 21:08:27,337 : INFO : EPOCH - 2 : training on 17359734 raw words (12392154 effective words) took 13.2s, 937376 effective words/s\n",
      "2019-01-01 21:08:28,350 : INFO : EPOCH 3 - PROGRESS: at 7.89% examples, 967878 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:29,366 : INFO : EPOCH 3 - PROGRESS: at 15.83% examples, 965213 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:30,369 : INFO : EPOCH 3 - PROGRESS: at 23.44% examples, 955452 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:31,378 : INFO : EPOCH 3 - PROGRESS: at 31.36% examples, 958591 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:32,381 : INFO : EPOCH 3 - PROGRESS: at 39.07% examples, 957266 words/s, in_qsize 7, out_qsize 1\n",
      "2019-01-01 21:08:33,382 : INFO : EPOCH 3 - PROGRESS: at 46.29% examples, 947019 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:34,388 : INFO : EPOCH 3 - PROGRESS: at 52.79% examples, 926218 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:35,391 : INFO : EPOCH 3 - PROGRESS: at 59.98% examples, 922995 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:36,399 : INFO : EPOCH 3 - PROGRESS: at 66.93% examples, 915605 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:37,401 : INFO : EPOCH 3 - PROGRESS: at 75.05% examples, 924028 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:38,409 : INFO : EPOCH 3 - PROGRESS: at 83.18% examples, 930689 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:39,411 : INFO : EPOCH 3 - PROGRESS: at 90.80% examples, 931997 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:40,415 : INFO : EPOCH 3 - PROGRESS: at 98.51% examples, 933486 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:40,601 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-01 21:08:40,603 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-01 21:08:40,607 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-01 21:08:40,608 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-01 21:08:40,609 : INFO : EPOCH - 3 : training on 17359734 raw words (12389271 effective words) took 13.3s, 933925 effective words/s\n",
      "2019-01-01 21:08:41,626 : INFO : EPOCH 4 - PROGRESS: at 6.99% examples, 860627 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:42,626 : INFO : EPOCH 4 - PROGRESS: at 14.57% examples, 893211 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:43,638 : INFO : EPOCH 4 - PROGRESS: at 22.59% examples, 922320 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:44,637 : INFO : EPOCH 4 - PROGRESS: at 30.58% examples, 939115 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:45,646 : INFO : EPOCH 4 - PROGRESS: at 38.50% examples, 944747 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:46,658 : INFO : EPOCH 4 - PROGRESS: at 46.00% examples, 941057 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:47,664 : INFO : EPOCH 4 - PROGRESS: at 52.67% examples, 924057 words/s, in_qsize 8, out_qsize 1\n",
      "2019-01-01 21:08:48,667 : INFO : EPOCH 4 - PROGRESS: at 57.88% examples, 890279 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-01 21:08:49,668 : INFO : EPOCH 4 - PROGRESS: at 62.87% examples, 860289 words/s, in_qsize 8, out_qsize 1\n",
      "2019-01-01 21:08:50,687 : INFO : EPOCH 4 - PROGRESS: at 69.07% examples, 849682 words/s, in_qsize 6, out_qsize 1\n",
      "2019-01-01 21:08:51,689 : INFO : EPOCH 4 - PROGRESS: at 76.04% examples, 850601 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:52,689 : INFO : EPOCH 4 - PROGRESS: at 83.18% examples, 853270 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:53,703 : INFO : EPOCH 4 - PROGRESS: at 89.98% examples, 852021 words/s, in_qsize 8, out_qsize 2\n",
      "2019-01-01 21:08:54,714 : INFO : EPOCH 4 - PROGRESS: at 96.53% examples, 848076 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:55,219 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-01 21:08:55,237 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-01 21:08:55,242 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-01 21:08:55,243 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-01 21:08:55,244 : INFO : EPOCH - 4 : training on 17359734 raw words (12391223 effective words) took 14.6s, 847184 effective words/s\n",
      "2019-01-01 21:08:56,272 : INFO : EPOCH 5 - PROGRESS: at 7.35% examples, 897574 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:57,282 : INFO : EPOCH 5 - PROGRESS: at 15.14% examples, 922226 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:58,284 : INFO : EPOCH 5 - PROGRESS: at 22.88% examples, 932158 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:08:59,287 : INFO : EPOCH 5 - PROGRESS: at 30.22% examples, 926644 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:09:00,294 : INFO : EPOCH 5 - PROGRESS: at 37.64% examples, 922456 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:09:01,297 : INFO : EPOCH 5 - PROGRESS: at 44.76% examples, 915610 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:09:02,308 : INFO : EPOCH 5 - PROGRESS: at 52.37% examples, 918734 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:09:03,308 : INFO : EPOCH 5 - PROGRESS: at 59.35% examples, 913398 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:09:04,312 : INFO : EPOCH 5 - PROGRESS: at 66.19% examples, 905718 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:09:05,317 : INFO : EPOCH 5 - PROGRESS: at 73.54% examples, 905803 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:09:06,328 : INFO : EPOCH 5 - PROGRESS: at 81.11% examples, 907499 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-01 21:09:07,345 : INFO : EPOCH 5 - PROGRESS: at 88.67% examples, 908849 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:09:08,361 : INFO : EPOCH 5 - PROGRESS: at 95.69% examples, 904183 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-01 21:09:08,909 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-01 21:09:08,913 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-01 21:09:08,924 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-01 21:09:08,926 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-01 21:09:08,926 : INFO : EPOCH - 5 : training on 17359734 raw words (12391608 effective words) took 13.7s, 906560 effective words/s\n",
      "2019-01-01 21:09:08,927 : INFO : training on a 86798670 raw words (61954655 effective words) took 68.5s, 904178 effective words/s\n",
      "2019-01-01 21:09:08,927 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-01-01 21:09:09,080 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2019-01-01 21:09:09,081 : INFO : not storing attribute vectors_norm\n",
      "2019-01-01 21:09:09,082 : INFO : not storing attribute cum_table\n",
      "2019-01-01 21:09:09,712 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print (\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentenceList, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2019-01-01 21:12:14,743 : WARNING : vectors for words {'jocund'} are not present in the model, ignoring these words\n",
      "E:\\Anaconda\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'enjoyable'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"good great jocund enjoyable\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = input(\"Sentence: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(forest.predict(vectorizer.transform([user]).toarray())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
